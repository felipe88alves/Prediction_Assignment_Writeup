---
title: "Prediction Assignment Writeup"
author: "Felipe Alves"
date: "7/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE, echo=FALSE}
library(caret)
library(dplyr)

# Based on https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md
library(parallel)
library(doParallel)
```

# Executive Summary

The goal of this report is to predict how well someone is performing a specific activity just by analyzing data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. This project is thus broken down into the following steps:

* Data Collection and Cleaning
* Evaluation of Prediction Models
* Selection of Best Prediction Model

# Data Collection and Cleaning

```{r data_collection, echo=FALSE}
training <- read.csv(file = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",header = TRUE,sep = ",")
testing <- read.csv(file = "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",header = TRUE,sep = ",")
```

The training and testing datasets are matrices of dimensions 19622x160 and 20x160. the only difference between the type of information is that the variable "classe", that we wish to predict in training, is labeled as "problem_id" in the test dataset.
A quick investigation of the data, using the *head*, *summary* and *str* commands revealed a couple of things:

1. There are a number of rows with NA's.
2. There are a number of rows with empty values.
3. The first variables are not relevant to prediction.

### 1 - Rows with NA's
An analysis of each column revealed that some variables have 19216 NA values. A decision was made here to remove these variables as only 2% (406 samples) of the data is actually populated for these variables.
Below are the names of the variables that were removed from the training and testing datasets due to a large number of NA's.
```{r removed_NAs, echo=FALSE}
removed_NAs <- sapply(training, function(x) length(which(!is.na(x)))) == 406
names(training[,removed_NAs])

training <- training[,!removed_NAs]
testing <- testing[,!removed_NAs]
```

### 2 - Rows with empty values.
Further analysis of the remaining columns revealed that other variables contained the same number of samples with empty values (19216). These variables were also removed from the training and testing datasets.
Below are the names of the variables that were removed from the training and testing datasets due to a large number of empty values.
```{r removed_Empty, echo=FALSE}
removed_Empty <- sapply(training, function(x) length(which(x != ""))) == 406
names(testing[,removed_Empty])

testing <- testing[,!removed_Empty]
training <- training[!removed_Empty]
```
### 3 - Irrelevant variables.
The first 7 variables are not relevant to the prediction of the class.
Below is the name of the variables that were removed from the datasets due to their low relevance.
```{r removed_irrel, echo=FALSE}
removed_irrel <- training[,1:7]
names(removed_irrel)

training <- training[,-c(1:7)]
testing <- testing[,-c(1:7)]
```

# Evaluation of Prediction Models

The final cleaned dataset that we have to work with contains `r dim(training)[1]` samples and `r dim(training)[2]` variables. this is the dataset we shall use to build our predictive models.

The training dataset was broken down into a training set and a cross validation set. The two set were generated by setting the seed to 1357 and by randomly allocating 75% of the data for the training set and 25% of the data for the cross-validation set.

```{r PartitionDataset}
set.seed(1357)
inTrain <- createDataPartition(y = training$classe, p=0.75, list = FALSE)
train_set <- training[inTrain,]; cross_val <- training[-inTrain,]
```

The training set will be used to create/train the different models and the cross validation set shall be used to evaluate each model.
The following prediction methods shall be tested:

* Random Forest
* Generalized Boosted Model (GBM)
 
### Random Forest
Note: Parallelization was used for the Random Forest model. The instructions on the parallel processing method used can be found in https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md

```{r parallel, echo=FALSE}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv",
                           number = 5,
                           allowParallel = TRUE)
```


```{r RF, echo=FALSE}
model_RF <- train(classe ~ ., data=train_set, method="rf",trControl=fitControl)
pred_RF <- predict(model_RF, newdata = cross_val)
confusionMatrix(pred_RF, cross_val$classe)
acc_rf <- confusionMatrix(pred_RF, cross_val$classe)
```

The accuracy of the model is of `r as.numeric(acc_rf)*100`%, that represents and out of sample error of `r (1-as.numeric(acc_rf))*100`%.

### Generalized Boosted Model
```{r GBM, echo=FALSE}
model_GBM <- train(classe ~ ., data=train_set, method="gbm", verbose = FALSE, trControl=fitControl)
model_GBM$finalModel
pred_GBM <- predict(model_GBM, newdata = cross_val)
confusionMatrix(pred_GBM, cross_val$classe)
acc_gmb <- confusionMatrix(pred_GBM, cross_val$classe)$overall[1]

# End Parallelization Cluster:
stopCluster(cluster)
registerDoSEQ()
```

The accuracy of the model is of `r as.numeric(acc_gmb)`%, that represents and out of sample error of `r (1-as.numeric(acc_gmb))`%.

# Selection of Best Prediction Model

Random Forest was selected as the preferred prediciton model as it displayed the highest accuracy results when predicting the **classe** variable of the cross-validation dataset.
The model was then used to predict the test set, the results are shown below
```{r test_prediction, echo=FALSE}
pred_Test <- predict(model_RF, newdata = testing)
pred_Test
```